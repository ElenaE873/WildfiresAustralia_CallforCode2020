{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Objective\n",
    "\n",
    "The ultimate aim of of the challenge is to **predict the area of wildfires in 7 regions in Australia for February 2021** with historical data, before they have happened! \n",
    "\n",
    "There are three submissions:\n",
    "- 1) Predict wildfires in February 2020.\n",
    "- 2) Predict wildifres in 3rd and 4th week of January 2021.\n",
    "- 3) Predict wildfires in February 20201."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Historical Weather\n",
    "\n",
    "This dataset contains daily aggregates computed from the hourly ERA5 climate reanalysis. Find more information about this data [here](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview) and [here](https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5)\n",
    "\n",
    "####Â Variables\n",
    "\n",
    "* All variables are aggregated to daily values from `YYYY-mm-ddT01:00:00Z` to `YYYY-mm-(dd+1)T00:00:00Z`\n",
    "* `Precipitation` is derived from total precipitation. Hourly raw data is converted from m/hour to mm/hour \n",
    "* [`Relative humidity`](https://en.wikipedia.org/wiki/Relative_humidity) is derived from the temperature and dewpoint\n",
    "* `Soil water content` is given for 0 - 7 cm below the surface\n",
    "* `Solar radiation`or Surface Solar Radiation Downwards. Units are converted from J/h to MJ/h\n",
    "* `Temperature`\n",
    "* `Wind speed` is calculated for every hour from the Easterly and Northerly 10 meter wind components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps:\n",
    "[1. Load Packages](#LoadPackages) \n",
    "\n",
    "[2. Descriptive Stats](#DescriptiveStats) \n",
    "\n",
    "[3. Evaluating for Missing Values(no missing values)](#MissingValues) \n",
    "\n",
    "[4. Checking for Duplicates (no duplicates)](#Duplicates) \n",
    "\n",
    "[5. Rearranging Table via Pivot](#PivotTable) \n",
    "\n",
    "[6. Evaluate Re-Arranced Parameter Columns for Missing and Duplicates](#RearrangedTable) \n",
    "\n",
    "[7. Weather Data Review](#DataReview) \n",
    "\n",
    "[8. Save out Pre-Processed \"C&P_Weather\" CSV File](#PreprocessedWeather) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load packages <a class=\"anchor\" id=\"LoadPackages\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages for analysis and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt, mpld3\n",
    "%matplotlib inline\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "from shapely.geometry import Polygon, mapping\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.plugins import TimeSliderChoropleth\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "\n",
    "* Data type has been changed to match across all other datasets.\n",
    "* Renaming columns to make more sense.\n",
    "* No null values.\n",
    "* No duplicates or drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "weather = \"H_Weather.csv\"\n",
    "print(\"Reading file: '{}'\".format(weather))\n",
    "weather_df = pd.read_csv(weather, parse_dates=[1])\n",
    "print(\"Loaded...\")\n",
    "\n",
    "# Columns and their datatypes\n",
    "print(weather_df.dtypes)\n",
    "\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "For every region {object}:\n",
    "\n",
    "    1 - Date : here is an object and will need to be defined as (format YYYY-MM-DD) {datetime64[ns]}\n",
    "    2 - Parameter includes: {object}\n",
    "\n",
    "            Precipiation (mmd/day)\n",
    "            Relative Humidity (%)\n",
    "            Soil water content (m3 m3)\n",
    "            Solar Radiation (MJ/day)\n",
    "            Temperature (C)\n",
    "            Wind speed (m/s)\n",
    "\n",
    "    3 - Count - (km2) {float64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing date type for consistency across all datasets\n",
    "weather_df['Date'] = pd.to_datetime(weather_df['Date'])\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "weather_cols = ['Date', 'Region', 'Parameter', 'area', 'min', 'max', 'mean', '2nd_moment']\n",
    "weather_df.columns= weather_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive Stats <a class=\"anchor\" id=\"DescriptiveStats\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating for Missing Values <a class=\"anchor\" id=\"MissingValues\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "weather_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for Duplicates <a class=\"anchor\" id=\"Duplicates\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find duplicates\n",
    "weather_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rows    : \", weather_df.shape[0])\n",
    "print(\"Columns : \", weather_df.shape[1])\n",
    "print(\"\\nFeatures : \", weather_df.columns.tolist())\n",
    "print(\"\\nMissing Values : \\n\",weather_df.isnull().any())\n",
    "print(\"\\nUnique Values : \\n\",weather_df.nunique())\n",
    "print(\"Number of records: {}\".format(len(weather_df)))\n",
    "print(\"Number of regions: {}\\n\".format(len(weather_df['Region'].unique())))\n",
    "print(weather_df['Region'].unique())\n",
    "print(weather_df['Parameter'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-arranging Table via Pivot Function <a class=\"anchor\" id=\"PivotTable\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns names\n",
    "weather_df.columns = ['Date', 'Region', 'Parameter', 'area', 'min', 'max', 'mean', '2nd_moment']\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rearranging Paramater values in the weather data\n",
    "df_pivot = weather_df.pivot_table(values=['min','max','mean','2nd_moment'], index=['Date','Region', 'area'], columns=['Parameter'])\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resetting the index on the new table formed\n",
    "df_pivot.reset_index(inplace=True)\n",
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming Column names\n",
    "df_pivot.columns = [col[0] if not(col[1]) else '{1}_{0}'.format(*col) for col in df_pivot.columns.values]\n",
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging Data and column\n",
    "params = df_pivot.columns.tolist()[3:]\n",
    "params.sort()\n",
    "weather_data = df_pivot[df_pivot.columns.tolist()[:3] + params].copy()\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows, num_cols = weather_data.shape\n",
    "print(\"There are total {} records in the following {} columns:\\n\".format(num_rows, num_cols))\n",
    "print(\"\\n\".join(list(weather_data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Re-Arranged Paramater Columns for Missing and Duplicates <a class=\"anchor\" id=\"RearrangedTable\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Check for null values in the weather data paramater columns now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking NULL values for - PRECIPITATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross checking null values in the new arranged data for the Precipitation column\n",
    "weather_data.loc[weather_data['Precipitation_mean'].isna(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifying the original data, that indeed there was no rain on 06-22-2019\n",
    "weather_df.loc[weather_df['Date'] == \"2019-06-22\", :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking NULL values for - TEMPERATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.loc[weather_data['Temperature_mean'].isna(), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking min value in the original data, as it appears to be null for two dates 02/08/2009 and 06/19/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.loc[weather_df['Date'] == \"2009-02-08\", :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.loc[weather_df['Date'] == \"2018-06-19\", :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that Temprature is null for two dates 2009-02-08 and 2018-06-19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also confirms data is properly arranged and checks out, meaning the null values exist because there are no readings for those column values in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find only the columns that have missing values\n",
    "null_columns = weather_data.columns[weather_data.isna().any()]\n",
    "weather_data[null_columns].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the index for missing values\n",
    "#weather_data[weather_data.isna().any(axis=1)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns DataFrame with missing values\n",
    "weather_data[weather_data.isna().any(axis=1)][null_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill null values with zeros\n",
    "weather_data = weather_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find duplicates\n",
    "weather_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather Data Review <a class=\"anchor\" id=\"DataReview\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequencies for  Region column\n",
    "weather_data.pivot_table(index= ['Region'], aggfunc='size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving out the final C&P_Weather CSV File <a class=\"anchor\" id=\"PreprocessedWeather\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_file = \"C&P_Weather.csv\"\n",
    "print(\"Saving file: '{}'\".format(final_file))\n",
    "weather_data.to_csv(final_file, index=False, encoding='utf-8')\n",
    "print(\"File Saved...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check DataFrame exported\n",
    "df = pd.read_csv(\"P:\\Wildfires_Australia\\cfc_wildfireforecastforAustralia\\C&P_Weather.csv\")\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Date.dtype.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
